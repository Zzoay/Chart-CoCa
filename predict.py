
import os

import math
import json
import random
import re
import shutil
import traceback

import numpy as np
import torch
import matplotlib.pyplot as plt
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig
from lmdeploy.vl import load_image
from tqdm import tqdm

from utils import process_axes
from constants import *


batch_size = 16 * torch.cuda.device_count()
from_scratch = False
repeat_sample_num = 30
dataset_path = '../CharXiv/'
info_path = os.path.join(dataset_path, 'data')
image_path = os.path.join(dataset_path, 'images')
model = '../InternVL/internvl_chat/work_dirs/internvl_chat_v2_0/InternVL2-8B'

instruction = """<IMAGE_TOKEN>\nGiven a chart, a question, and several answers which may contain the correct one, your task is: 
If there is a correct response, output it. If not, generate the correct answer.
"""

def descriptive_query_helper(qid, subplot_loc):
    if qid in [18, 19]:
        # skip subplot location when asking about the layout of the subplots
        return DESCRIPTIVE_RESP_INST[qid] 
    if isinstance(subplot_loc, list):
        if subplot_loc[0] == 0:
            # when there is only one subplot
            prefix = "For the current plot, "
        else:
            # when there are multiple subplots
            prefix = f"For the subplot at row {subplot_loc[0]} and column {subplot_loc[1]}, "
    # when subplots do not form a grid
    elif isinstance(subplot_loc, str):
        prefix = f"For {subplot_loc}, "
    else:
        raise ValueError(f"Invalid subplot_loc: {subplot_loc}")
    # return the question with the subplot location
    return DESCRIPTIVE_RESP_INST[qid].format(prefix)

def predict_desc():
    print("Descriptive task")
    data_info = json.load(open(os.path.join(info_path, f'descriptive_val.json')))
    data_ids = [int(d) for d in data_info]
    images = [load_image(os.path.join(image_path, f'{d}.jpg')) for d in data_ids]

    pipe = pipeline(model, backend_config=TurbomindEngineConfig(session_len=20480, tp=torch.cuda.device_count(), cache_max_entry_count=0.9))

    # get multiple responses generated by the original llm
    ans_data = []
    for repeat_i in range(repeat_sample_num):
        ans_data.append(json.load(open(os.path.join(dataset_path, f"results/mul-internvl/gen-InternVL2-8B-descriptive_val-{repeat_i}.json"))))

    ret = {}
    queries = []
    for d_id in data_info:
        item = data_info[d_id]
        img_id = item['figure_id']
        subplot_loc = item['subplot_loc']
        for idx, qid in enumerate(item['qids']):
            question = descriptive_query_helper(qid, subplot_loc)
            ret[f"{d_id}_{idx}"] = {"figure_id": item['figure_id'], "subq_idx": idx, "qid": qid}
            mul_responses = []
            for repeat_i in range(repeat_sample_num):
                # mul_responses.append(f"{repeat_i+1}. " + ans_data[repeat_i][f"{d_id}_{idx}"]["response"])
                mul_responses.append(ans_data[repeat_i][f"{d_id}_{idx}"]["response"])

            mul_responses_str = "\n".join(f"Response {i+1}:\n{x}\n" for i,x in enumerate(mul_responses))
            # qst_str = "Question:\n" + question + "\nResponses:\n\n" + "\n".join(mul_responses)
            qst_str = "\nQuestion:\n" + question + "\nResponses:\n\n" + mul_responses_str
            # print(f"Q{qid}: {question}")
            queries.append(instruction + qst_str)
    
    # get critic
    for i in tqdm(range(0, len(data_ids), batch_size)):
        batch_ids = data_ids[i:i+batch_size]
        batch_images = images[i:i+batch_size]
        batch_queries = []
        for idx in range(i, i + batch_size):
            start_idx = idx * 4
            batch_queries.extend(queries[start_idx:start_idx + 4])

        expanded_images = [img for img in batch_images for _ in range(4)]
        batch_prompts = [(query, img) for query, img in zip(batch_queries, expanded_images)]
        batch_responses = pipe(batch_prompts, gen_config=GenerationConfig(max_new_tokens=512, do_sample=True, top_p=0.6))
        batch_critics = [response.text for response in batch_responses]
        for j, d_id in enumerate(batch_ids):
            for idx in range(4): 
                ret[f"{d_id}_{idx}"]["response"] = batch_critics[j*4+idx]
    
    # save to json
    with open(os.path.join(dataset_path, f"results/crit/gen-InternVL2-8B-descriptive_val.json"), 'w') as f:
        json.dump(ret, f, indent=4, ensure_ascii=False)

def get_number_instruction(answer):
    base = answer.split('.')
    whole, decimal = base[0], None if len(base) == 1 else base[1]
    # check if it contains decimal places
    if whole is not None and decimal is None:
        inst = "* Your final answer must be an exact integer."
    elif whole is not None and decimal is not None:
        num_decimal = len(decimal)
        inst = f"* Your final answer must be a number with {num_decimal} decimal places."
    else:
        raise ValueError(f"Invalid answer: {answer}")
    return inst

def predict_reason():
    print("Reasoning task")
    data_info = json.load(open(os.path.join(info_path, f'reasoning_val.json')))
    data_ids = [int(d) for d in data_info]
    images = [load_image(os.path.join(image_path, f'{d}.jpg')) for d in data_ids]

    pipe = pipeline(model, backend_config=TurbomindEngineConfig(session_len=20480, tp=torch.cuda.device_count(), cache_max_entry_count=0.9))

    # get multiple responses generated by the original llm
    ans_data = []
    for repeat_i in range(repeat_sample_num):
        ans_data.append(json.load(open(os.path.join(dataset_path, f"results/mul-internvl/gen-InternVL2-8B-reasoning_val-{repeat_i}.json"))))

    ret = {}
    queries = []
    for d_id in data_info:
        item = data_info[d_id]
        # for idx, qid in enumerate(item['qids']):
        # question = descriptive_query_helper(qid, subplot_loc)
        inst_category = item['inst_category']
        # 1: text-in-chart, 2: text-in-general, 3: number-in-chart
        if inst_category in [1, 2, 3]:
            question = REASONING_RESP_INST[inst_category].format(item['query'])
        # 4: number-in-general -> need to specify the number of decimal places
        elif inst_category == 4:
            question = REASONING_RESP_INST[inst_category].format(item['query'], \
                                        get_number_instruction(item['answer']))
            
        ret[f"{d_id}"] = {"figure_id": item['figure_id'], "inst_category": item['inst_category'], "raw_question": item['query']}
        mul_responses = []
        for repeat_i in range(repeat_sample_num):
            # mul_responses.append(f"{repeat_i+1}. " + ans_data[repeat_i][f"{d_id}"]["response"])
            mul_responses.append(ans_data[repeat_i][f"{d_id}"]["response"])

        mul_responses_str = "\n".join(f"Response {i+1}:\n{x}\n" for i,x in enumerate(mul_responses))
        qst_str = "\nQuestion:\n" + question + f"\n\nResponses:\n\n" + mul_responses_str
        # qst_str = "Question:\n" + question + "\nResponses:\n\n" + "\n".join(mul_responses)
        # print(f"Q{qid}: {question}")
        queries.append(instruction + qst_str)
    
    # get critic
    for i in tqdm(range(0, len(data_ids), batch_size)):
        batch_ids = data_ids[i:i+batch_size]
        batch_images = images[i:i+batch_size]
        batch_queries = queries[i:i+batch_size]

        assert len(batch_queries) == len(batch_images)
        # expanded_images = [img for img in batch_images for _ in range(4)]
        batch_prompts = [(query, img) for query, img in zip(batch_queries, batch_images)]
        batch_responses = pipe(batch_prompts, gen_config=GenerationConfig(max_new_tokens=512, do_sample=True, top_p=0.6))
        batch_critics = [response.text for response in batch_responses]
        for j, d_id in enumerate(batch_ids):
            ret[f"{d_id}"]["response"] = batch_critics[j]
    
    # save to json
    with open(os.path.join(dataset_path, f"results/crit/gen-InternVL2-8B-reasoning_val.json"), 'w') as f:
        json.dump(ret, f, indent=4, ensure_ascii=False)


if __name__ == '__main__':
    predict_desc()
    predict_reason()
